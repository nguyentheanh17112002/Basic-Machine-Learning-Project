{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import  datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight and bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear weights: Parameter containing:\n",
      "tensor([[ 0.0102, -0.0086,  0.0224,  ..., -0.0346, -0.0233, -0.0066],\n",
      "        [ 0.0111,  0.0303, -0.0164,  ...,  0.0018, -0.0163, -0.0344],\n",
      "        [ 0.0015,  0.0086,  0.0091,  ...,  0.0345,  0.0179, -0.0104],\n",
      "        ...,\n",
      "        [-0.0208, -0.0211, -0.0145,  ..., -0.0247, -0.0348, -0.0109],\n",
      "        [ 0.0156, -0.0279, -0.0284,  ...,  0.0339,  0.0141,  0.0228],\n",
      "        [-0.0340,  0.0063, -0.0136,  ..., -0.0156, -0.0027, -0.0076]],\n",
      "       requires_grad=True) \n",
      "\n",
      "First Linear weights: Parameter containing:\n",
      "tensor([ 1.2861e-02,  1.3429e-02,  2.1347e-02, -1.0868e-02,  2.4888e-02,\n",
      "         2.5401e-02,  2.8965e-02,  8.5281e-03,  1.1845e-02,  1.9359e-02,\n",
      "         1.0021e-02, -4.4040e-03,  6.3979e-03, -1.2354e-02,  8.2412e-03,\n",
      "        -2.5957e-02,  2.8524e-02,  1.2381e-02,  2.2658e-02, -2.7716e-02,\n",
      "        -1.4372e-02, -2.3680e-02, -1.9660e-02, -1.6915e-02,  1.7663e-02,\n",
      "        -1.2699e-02,  1.8727e-02,  3.0831e-02, -5.2370e-03, -8.9270e-03,\n",
      "         3.3359e-02,  5.3685e-03, -1.1596e-02,  8.7732e-03,  2.3269e-02,\n",
      "         7.4115e-03,  1.8519e-02,  3.0804e-02, -5.9809e-03,  3.5273e-02,\n",
      "         2.7911e-02, -2.9175e-02,  2.7973e-02, -6.4658e-03,  3.3640e-02,\n",
      "        -1.9816e-02, -1.6931e-02, -1.3671e-02, -3.1535e-02, -2.5679e-02,\n",
      "        -2.0860e-02, -2.8624e-02,  7.7581e-03,  2.6776e-02,  3.2478e-02,\n",
      "        -1.6592e-02, -9.9831e-03,  3.0155e-02, -2.5089e-02,  2.8077e-02,\n",
      "         8.3857e-03,  2.9850e-02,  1.2437e-02, -4.7318e-03,  3.3138e-02,\n",
      "         2.5447e-02,  2.5097e-02,  3.3151e-02, -1.4774e-02, -3.8171e-03,\n",
      "         3.3186e-02,  2.9787e-03,  2.4389e-05,  2.8814e-03,  2.3314e-02,\n",
      "         1.6512e-02, -2.4841e-02, -1.2747e-02,  3.2643e-03, -1.4182e-02,\n",
      "         1.2678e-02, -1.0030e-02,  1.3360e-02,  2.3650e-02,  1.1010e-02,\n",
      "         1.9635e-02, -7.9457e-03, -2.9454e-02,  1.6463e-02, -1.3798e-04,\n",
      "        -2.4691e-02, -1.2738e-02,  1.2545e-02, -7.8736e-03, -1.3997e-02,\n",
      "         1.6305e-02, -2.0894e-02,  2.4247e-03, -3.4064e-03,  4.1820e-03,\n",
      "         2.9118e-02,  2.7056e-02,  2.6463e-02,  1.2311e-02,  3.1557e-02,\n",
      "        -9.9016e-03, -3.0710e-02,  3.0434e-03, -1.3522e-02, -1.8149e-02,\n",
      "         1.2711e-03,  3.4771e-02,  8.1170e-04,  7.1688e-03, -2.9981e-03,\n",
      "         2.6533e-03, -1.4959e-02, -2.0369e-02, -3.5336e-02, -4.3437e-03,\n",
      "        -8.9137e-03, -2.2664e-02,  3.5333e-02, -6.0220e-03,  1.4740e-04,\n",
      "         9.7946e-03, -4.5918e-03,  2.0232e-02,  3.0233e-02, -2.0668e-02,\n",
      "        -3.3455e-02, -1.8548e-02, -2.8286e-02, -2.2642e-02, -2.4731e-02,\n",
      "         2.5996e-02,  1.7776e-02,  1.7801e-02, -1.7999e-02, -1.7183e-02,\n",
      "        -3.1484e-02,  1.6224e-02, -7.1203e-03, -2.4877e-02, -1.2248e-02,\n",
      "         1.7778e-02, -2.9883e-02,  1.4406e-02, -1.1425e-02,  1.5116e-02,\n",
      "         1.5916e-02,  6.1673e-03,  3.4845e-02, -8.5945e-03,  1.2505e-02,\n",
      "         2.6416e-02,  8.5515e-03, -1.6573e-02, -2.0346e-02, -6.3795e-03,\n",
      "        -2.9241e-02,  5.4550e-03,  1.7046e-02, -1.5152e-02, -5.7093e-03,\n",
      "         5.3687e-03, -3.0974e-02, -1.8556e-02,  1.6872e-03,  2.0228e-02,\n",
      "        -1.3356e-02,  2.2399e-02,  2.7629e-02, -3.3739e-02, -7.9942e-03,\n",
      "        -4.4704e-03, -1.8751e-02,  1.4429e-02, -2.4191e-02, -2.8014e-02,\n",
      "         8.4238e-03, -8.5289e-03,  7.8749e-03, -1.5297e-02,  1.2243e-03,\n",
      "        -5.7098e-03,  2.2658e-02,  1.9736e-02,  1.9602e-02,  1.2239e-02,\n",
      "         3.8669e-03, -2.2647e-02, -3.3205e-02,  2.6546e-02, -3.4712e-02,\n",
      "        -2.8574e-02, -2.0192e-02, -1.9870e-02,  5.0272e-03, -3.9619e-03,\n",
      "         1.8957e-02,  1.4231e-02,  1.8379e-02, -1.5805e-02,  2.2214e-02,\n",
      "        -3.5148e-02,  2.4040e-02, -3.2003e-02,  4.3716e-03, -2.8422e-02,\n",
      "         1.9425e-02, -2.5591e-02, -2.6203e-02,  1.4144e-02,  7.1898e-03,\n",
      "         2.0206e-03,  1.6085e-02, -5.6250e-03,  2.8969e-02,  2.8983e-02,\n",
      "        -1.5320e-02, -2.2866e-02,  8.3753e-03, -3.1793e-02,  2.4052e-02,\n",
      "         3.2295e-02,  8.1278e-03,  5.4085e-03,  3.3852e-02,  2.6697e-02,\n",
      "         1.1804e-02, -7.1225e-03,  2.4262e-03,  3.4275e-02, -2.7119e-02,\n",
      "        -2.6671e-02,  1.8556e-02, -2.6910e-03, -2.0466e-02,  2.3393e-02,\n",
      "         2.6208e-02, -3.8616e-03,  1.5342e-02, -2.1027e-02, -4.2269e-03,\n",
      "         1.3460e-02, -7.7623e-03, -1.9500e-03,  8.7416e-03, -3.9026e-03,\n",
      "        -4.5843e-03, -1.7902e-02, -1.3759e-02, -2.9839e-02,  1.9306e-02,\n",
      "        -1.5342e-02, -2.8343e-03, -1.4189e-02,  1.4392e-02, -2.4118e-02,\n",
      "         2.1489e-03, -2.8781e-02, -3.8182e-03, -2.8064e-02,  4.8708e-03,\n",
      "        -2.6249e-02,  3.4178e-02, -2.0555e-03,  2.2956e-02,  3.3944e-02,\n",
      "        -9.9996e-03,  9.4557e-03,  6.4237e-03,  1.9961e-02,  2.5031e-02,\n",
      "        -1.2018e-02,  3.1709e-02, -3.4426e-03,  2.7282e-02, -2.7405e-03,\n",
      "        -1.3762e-02,  1.7045e-02, -2.3337e-02, -2.6636e-02,  6.3161e-03,\n",
      "         3.3647e-02, -1.5838e-02,  2.2304e-02,  2.3643e-02, -5.1085e-03,\n",
      "        -1.1057e-02,  1.4097e-02,  3.2449e-02,  3.0721e-02,  2.7409e-02,\n",
      "         2.0583e-02,  2.5649e-02,  7.7190e-03,  8.2193e-04,  1.8320e-02,\n",
      "         3.1841e-02, -2.6689e-02,  2.5577e-02,  1.6373e-04, -1.0439e-02,\n",
      "        -3.0824e-02,  2.5049e-02,  8.5888e-03,  2.5948e-03, -3.1587e-02,\n",
      "        -3.1240e-02,  2.0066e-02,  6.8483e-03, -8.0124e-03, -7.9142e-03,\n",
      "        -3.3968e-02,  1.8220e-02, -2.2073e-02,  2.5229e-02, -2.0387e-02,\n",
      "        -1.0091e-02,  3.9938e-03,  2.8327e-02,  1.2176e-02, -8.1465e-03,\n",
      "         1.3639e-02,  2.3127e-02, -1.2192e-02, -8.8149e-03, -3.1952e-02,\n",
      "         1.8474e-02, -1.9725e-02,  2.3726e-02, -1.0030e-02,  3.3516e-02,\n",
      "        -1.3114e-02,  3.1912e-02,  1.1502e-02,  1.6168e-02,  3.1493e-02,\n",
      "         3.3621e-02,  1.3791e-02, -2.0153e-02,  1.1749e-02, -1.5859e-02,\n",
      "        -2.3561e-02,  1.5217e-02,  2.4036e-02, -5.2043e-04,  3.0125e-02,\n",
      "         1.0535e-02, -1.9460e-03, -7.2264e-03, -1.4521e-02, -2.6762e-02,\n",
      "         2.6420e-02,  1.4648e-02,  2.0477e-02, -1.6686e-02, -1.7151e-02,\n",
      "        -2.5337e-02, -1.4090e-02, -3.1725e-02, -3.1292e-02,  3.1887e-02,\n",
      "        -1.4206e-02, -2.3395e-03,  9.3418e-03, -1.6480e-02,  1.1328e-02,\n",
      "         2.0491e-02, -3.4959e-02,  8.2878e-03,  1.2984e-02, -1.7275e-02,\n",
      "        -5.6803e-03,  1.0593e-03,  7.8580e-03, -2.2264e-02,  6.2227e-03,\n",
      "         2.6227e-02, -2.3450e-02, -2.2849e-02,  2.2658e-02,  2.6067e-03,\n",
      "         1.6660e-02, -2.5090e-02,  2.4468e-03,  3.5119e-02, -2.0970e-02,\n",
      "         1.6488e-02,  2.2364e-02,  1.8891e-02, -1.3107e-02,  9.6684e-03,\n",
      "         6.4799e-03, -2.8913e-02, -9.4777e-03,  2.3748e-02, -1.5566e-02,\n",
      "        -2.4309e-04, -2.9184e-02, -3.5414e-03,  9.1277e-03, -1.9100e-02,\n",
      "        -9.2492e-03, -9.5184e-03,  8.7909e-03,  1.7305e-02, -1.8032e-02,\n",
      "         2.4405e-02,  3.2275e-02, -3.1037e-02, -2.4911e-02, -1.4903e-02,\n",
      "         2.6966e-02,  3.5249e-03,  1.8553e-02,  1.1330e-02,  6.6820e-03,\n",
      "         2.7100e-02, -1.0427e-02, -6.3659e-03, -1.1751e-02,  3.0612e-02,\n",
      "        -6.8996e-04, -1.0657e-02, -1.2502e-02,  3.8256e-03,  3.3445e-02,\n",
      "        -5.5579e-03, -3.7559e-03, -6.2666e-03, -3.4734e-02,  3.4465e-02,\n",
      "         3.3085e-02, -4.9356e-03,  1.3259e-02,  3.8580e-03,  5.8102e-03,\n",
      "         3.0288e-02, -1.6667e-02,  1.4706e-02,  3.0565e-03, -1.4177e-03,\n",
      "         1.1795e-02, -1.1384e-02,  3.0154e-02,  3.3204e-02, -2.7880e-02,\n",
      "        -3.5367e-02, -2.8705e-02,  1.1398e-02,  2.6285e-02, -2.5908e-02,\n",
      "         3.4515e-02,  2.4871e-02, -4.3158e-03,  4.7671e-03,  2.8417e-03,\n",
      "        -2.7566e-02, -2.2338e-02,  3.4306e-02,  3.1999e-02, -2.8089e-02,\n",
      "         9.0883e-03,  2.4902e-02,  2.4501e-02,  3.5102e-02, -8.7342e-03,\n",
      "        -2.8551e-02,  1.1756e-03, -9.7969e-03, -2.9429e-02,  1.8130e-02,\n",
      "         2.9893e-02,  2.1689e-02, -1.0074e-02, -3.2115e-02,  9.6890e-03,\n",
      "         2.4546e-02,  1.0533e-03, -2.4871e-02,  2.6050e-02,  1.3930e-02,\n",
      "         2.5849e-02,  1.5227e-02, -1.7891e-02,  2.4694e-02,  2.1749e-02,\n",
      "        -7.5606e-03,  2.9269e-02, -5.3660e-03,  1.7107e-02,  5.8445e-03,\n",
      "        -5.8444e-03, -1.7894e-02,  9.7399e-03, -9.8860e-03,  2.9653e-02,\n",
      "         6.7156e-03,  3.3891e-02, -2.1354e-02,  1.9587e-02, -3.2037e-02,\n",
      "        -1.1861e-02, -3.5758e-03,  3.5462e-02, -1.7171e-02,  3.3253e-02,\n",
      "         2.1769e-02, -7.1263e-03], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
    "\n",
    "print(f\"First Linear weights: {model.linear_relu_stack[0].bias} \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand((3,28,28))\n",
    "print(input_image.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flatten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# nn.Linear"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=512)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# nn.ReLU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.3874,  0.7838,  0.0600,  ...,  0.2989, -0.3200, -0.2238],\n",
      "        [ 0.3026,  0.3714,  0.3622,  ...,  0.3738, -0.1051,  0.3480],\n",
      "        [ 0.4368,  0.3280,  0.5274,  ...,  0.1994, -0.3828, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.3874, 0.7838, 0.0600,  ..., 0.2989, 0.0000, 0.0000],\n",
      "        [0.3026, 0.3714, 0.3622,  ..., 0.3738, 0.0000, 0.3480],\n",
      "        [0.4368, 0.3280, 0.5274,  ..., 0.1994, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# nn.Sequential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x512 and 20x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m seq_modules \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m      2\u001B[0m     flatten,\n\u001B[0;32m      3\u001B[0m     layer1,\n\u001B[0;32m      4\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(),\n\u001B[0;32m      5\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m input_image \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m28\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m logits \u001B[38;5;241m=\u001B[39m seq_modules(input_image)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (3x512 and 20x10)"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
